<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jiahao Lyu</title>

    <meta name="author" content="Jiahao Lyu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiahao Lyu
                </p>
                <p>I'm a PhD student at <a href="https://www.iie.ac.cn/">IIE, CAS</a> in Beijing, China.
                  At IIE, I've worked on <a href="https://intimelab.github.io/">InTimeLab</a>, IIE, CAS, advised by Professor Yu Zhou and Can Ma.
                  Before that, I got a Bachelor`s degree from Beijing Normal University.
                </p>
                <p style="text-align:center">
                  <a href="mailto:lvjiahao@iie.ac.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=aDWATbsAAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/pd162/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/favicon/ljh.jpg"><img style="width:50%;max-width:100%;object-fit: cover; border-radius: 25%;" alt="profile photo" src="images/favicon/ljh.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>
                  News
                </h2>
                <ul>
		  <li>
                      2025/06: A co-first-author paper has been accepted by ICDAR 2025.
                  </li>
		  <li>
                      2025/05: A first-author paper has been accepted by TOMM 2025.
                  </li>
		  <li>
                      2025/04: A co-first-author paper has been accepted by IJCAI 2025.
                  </li>
		  <li>
                      2025/03: Selected as a Reviewer of IJCAI 2025.
                  </li>
		  <li>
                      2025/02: Selected as a Reviewer of ICDAR 2025.
                  </li>
                  <li>
                      2024/12: A first-author paper has been accepted by AAAI2025.
                  </li>
                  <li>
                    2024/12: A cooperative paper has been accepted by ICASSP2025.
                  </li>
                  <li>
                    2023/06: A cooperative paper has been accepted by PRCV2023.
                  </li>
                  <li>
                    2022/12: Third Prize, Channel of street text detection and recognition, 1st International Algorithm Cases
                    Competition, Pazhou Lab, Huangpu.
                  </li>
                  <li>
                    2022/06: A cooperative paper has been accepted by ACM MM2022.
                  </li>
                </ul>
                <h2>Research</h2>
                <p>
                  I'm interested in Scene Text Detection, Recognition & Spotting. Below are some selected publications.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
     <tr onmouseout="lsg_stop()" onmouseover="lsg_start()" bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='data/LTB.png' width="160" height="auto" style="margin-top: 20px;">
        </div>
        <!-- <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script> -->
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="">
			<span class="papertitle">The Devil is in Fine-tuning and Long-tailed Problems: A New Benchmark for Scene Text Detection
</span>
        </a>
        <br>
				<a href="">Tianjiao Cao</a>,
				<a href=""><strong>Jiahao Lyu</strong></a>,
				<a href="">Weichao Zeng</a>,
				<a href="">Weiming Mu</a>,
				<a href="">Yu Zhou</a>,
        <br>
        <em>IJCAI</em>, 2025
        <br>
        <a href="https://github.com/pd162/LTB">Code</a>
        /
        <a href="https://arxiv.org/abs/2505.15649">arXiv</a>
        <p></p>
        <p>
		We uncover two key factors contributing to this discrepancy through extensive scene text detection experiments, Fine-tuning Gap and long-tailed distribution of texts we advocate for a Joint-Dataset Learning (JDL) protocol to alleviate the Fine-tuning Gap. Additionally, an error analysis is conducted to identify three major categories and 13 subcategories of challenges in long-tailed scene text, upon which we propose a Long-Tailed Benchmark (LTB). 
        </p>
      </td>
    </tr>
		  
    <tr onmouseout="lsg_stop()" onmouseover="lsg_start()" bgcolor="#ffffff">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='data/architecture.jpg' width="160" height="auto" style="margin-top: 20px;">
        </div>
        <!-- <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script> -->
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.10159">
			<span class="papertitle">Arbitrary Reading Order Scene Text Spotter with Local Semantics Guidance
</span>
        </a>
        <br>
				<a href=""><strong>Jiahao Lyu</strong></a>,
				<a href="">Wei Wang</a>,
				<a href="">Dongbao Yang</a>,
				<a href="">Jinwen Zhong</a>,
				<a href="">Yu Zhou</a>,
        <br>
        <em>AAAI</em>, 2025
        <br>
        <a href="https://github.com/pd162/LSG/">Code</a>
        /
        <a href="https://arxiv.org/abs/2412.10159">arXiv</a>
        <p></p>
        <p>
				We propose LSGSpotter, a local semantics-guided scene text spotter to handle the arbitrary reading order text instances without sophisticated detection.
        </p>
      </td>
    </tr>

    <tr onmouseout="lsg_stop()" onmouseover="lsg_start()" bgcolor="#ffffff">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='data/charsam.jpg' width="160" height="100" style="margin-top: 20px;">
        </div>
        <!-- <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script> -->
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.19917">
			<span class="papertitle">Char-SAM: Turning Segment Anything Model into Scene Text Segmentation Annotator with Character-level Visual Prompts
</span>
        </a>
        <br>
        <a href="">Enze Xie</a>
				<a href=""><strong>Jiaho Lyu</strong></a>,
				<a href="">Daiqing Wu</a>,
				<a href="">Huawen Shen</a>,
				<a href="">Yu Zhou</a>,
        <br>
        <em>ICASSP</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2412.19917">arXiv</a>
        <p></p>
        <p>
          We propose an automatic annotation pipeline named Char-SAM, that turns SAM into a low-cost segmentation annotator with a Character-level visual prompt.
        </p>
      </td>
    </tr>

    <tr onmouseout="lsg_stop()" onmouseover="lsg_start()" bgcolor="#ffffff">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='data/textblockv2.jpg' width="160" height="100" style="margin-top: 20px;">
        </div>
        <!-- <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script> -->
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2403.10043">
			<span class="papertitle">TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model
</span>
        </a>
        <br>
				<a href=""><strong>Jiaho Lyu</strong></a>,
				<a href="">Jin Wei</a>,
				<a href="">Gangyan Zeng</a>,
				<a href="">Zeng Li</a>,
        <a href="">Enze Xie</a>,
        <a href="">Wei Wang</a>,
        <a href="">Can Ma</a>,
        <a href="">Yu Zhou</a>,
        <br>
        <em>TOMM</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2403.10043">arXiv</a>
        <p></p>
        <p>
          Extension of <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548051">TextBlock (ACM MM2022)</a>
        </p>
      </td>
    </tr>

          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
